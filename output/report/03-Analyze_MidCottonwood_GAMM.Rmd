---
title: "Application of GAMM to Middle Cottonwood Trail"
author: "Meridith L. Bartley, PhD"
date: "`r Sys.Date()`"
header-includes:
  - \usepackage{booktabs}
  - \usepackage{caption}
  - \usepackage{longtable}
  - \usepackage{tabu}
  - \usepackage{floatrow}
  - \usepackage{subfig}
  - \usepackage{lineno}
  - \floatsetup[table]{capposition=top}
  - \usepackage{pdflscape}
  - \newcommand{\blandscape}{\begin{landscape}}
  - \newcommand{\elandscape}{\end{landscape}}
output:
  bookdown::pdf_document2: default  # fig_caption: yes
  bookdown::html_document2: default
always_allow_html: yes
---

```{r, echo=FALSE, message=F, warning=F}
library(dplyr)
library(here)
library(ggplot2)
library(gratia)
library(itsadug)
library(lubridate)
library(magrittr)
library(marginaleffects)
library(mgcv)
library(xtable)

source(here("scripts/01-LoadData.R"))

#helps to format this document 
#by default the included code chunks do not appear in the report view
knitr::opts_chunk$set(echo = F, 
                      eval = F,
                      message = F, 
                      cache=TRUE, 
                      warning=FALSE, 
                      error=FALSE, 
                      root.dir = normalizePath(".."))

```

```{r}
## Model Checking function
tsDiagGamm <- function(x, timevar, observed, f = 0.3, type = "normalized") {
  resi <- resid(x$lme, type = type)
  fits <- fitted(x$lme)
  on.exit(layout(1))
  layout(matrix(1:6, ncol = 3, byrow = TRUE))
  plot(resi ~ fits, ylab = "Normalized Residuals",
       xlab = "Fitted Values", main = "Fitted vs. Residuals")
  lines(lowess(x = fits, y = resi, f = f), col = "blue",
        lwd = 2)
  plot(resi ~ timevar, ylab = "Normalized Residuals",
       xlab = "Time", main = "Time series of residuals")
  lines(lowess(x = timevar, y = resi, f = f), col = "blue", lwd = 2)
  plot(observed ~ fits, ylab = "Observed",
       xlab = "Fitted Values", main = "Fitted vs. Observed",
       type = "n")
  abline(a = 0, b = 1, col = "red")
  points(observed ~ fits)
  lines(lowess(x = fits, y = observed, f = f), col = "blue",
        lwd = 2)
  hist(resi, freq = FALSE, xlab = "Normalized Residuals")
  qqnorm(resi)
  qqline(resi)
  acf(resi, main = "ACF of Residuals")
}
```

# Application to Middle Cottonwood Trail

The aim of this project is to understand and predict the pattern of trail use recorded at multiple multi-use trails in the Bridger Mountains. We want to be able to predict trail use over a full calendar year and in locations where no trail counter data exist, within the Bridgers and nearby trails. In this report we start off with a simple `gam` model and explore gradually more complicated versions which we apply to a single trail (rather than then entire network of trails) so we can focus on understanding the modelling aspect for now.

We being with a quick look at the data available for our chosen trail, Middle Cottonwood Trail. Then we will progress through several `ga(m)m` models that incorporate more complexity with each iteration. This will include a detailed overview of useful diagnostic tools, a look at what explanatory predictor variables should/could be included, as well as an example of how we may forecast (predict into the near future) using our chosen model.

## The Data

```{r combine-data , eval = T}
## Focusing on Middle Cottenwood Trail (Trail # 586, Counter ID # 31)

singleCount <- trail_count %>%
  dplyr::filter(counterid == 31)

singleStrava <- strava_day %>%
  dplyr::filter(trailnumber == 586) %>% 
  # we want a single representation count value for multiple edges per day
  dplyr::group_by(timeframe) %>% 
  dplyr::mutate(max.count = max(totaltrips)) %>% 
  dplyr::select(-c(totaltrips, totalpeople, edge_uid)) %>% 
  dplyr::distinct()
# trail characteristics not pertinent for single trail analysis
# no variation within trail information

# no variation within trail information

## Combine Camera Counts, Strava Counts, Weather, Trail Characteristics

singleTrail <- singleCount %>%
  dplyr::left_join(
    singleStrava,
    by = c(
      "date" = "timeframe",
      "trailnumber" = "trailnumber",
      "trailname" = "trailname",
      "wday" = "wday"
    )
  ) %>%
  dplyr::left_join(weather, by = c("date" = "date"))

## Add in a Time variable
singleTrail$yday <- lubridate::yday(singleTrail$date)

## Assume missing Strava data reflect a 0 observation

singleTrail$max.count[which(is.na(singleTrail$max.count))] <- 0

## remove unecessary columns
singleTrail <- singleTrail %>% 
  dplyr::select(-c("counterid",
                                "trailnumber",
                                "subsectionname", 
                                "counterowner",
                                "th_lat",
                                "th_long",
                                "counter_lat", 
                                "counter_long",
                                "stravaedgeid2022"))

#convert week and wday to integers now
singleTrail$wday <- as.integer(singleTrail$wday)
singleTrail$week <- as.integer(singleTrail$week)


## remove rows with NA in temp_min_F (only NA in covariates used in model fitting)
singleTrail <- singleTrail[-49, ]


## gather strava+covariate data w/out camera counter data for predicting

predict.data <- singleStrava %>% 
  dplyr::left_join(weather, by = c("timeframe" = "date")) %>% 
  dplyr::filter(timeframe %notin% singleCount$date) %>% 
  dplyr::mutate(yday = lubridate::yday(timeframe)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(c("yday", 
                  "wday", 
                  "max.count", 
                  "month", 
                  "precipitation_in", 
                  "temp_min_f", 
                  "daily_aqi_value"))

#convert week and wday to integers now
predict.data$wday <- as.integer(predict.data$wday)
# predict.data$week <- as.integer(predict.data$week)


```

This 2.3-mile out-and-back trail was monitored via infrared counters between `r min(singleTrail$date)` and `r max(singleTrail$date)`. This daily time series count data is combined with data from Strava Metro, and local weather data.

Strava Metro partners with Headwaters Economics to provide aggregated (daily, monthly, or annually) trail use information for the Bridgers. These data are broken into multiple edges within a single trail. Eight such edges are included for Middle Cottonwood. These edges do not represent independent observations as they represent trail use on connected locations and thus likely reflect the same trail users continuing along the trail. We summarize these data to a single value using the edge ID with the maximum observed number of trail numbers for a given day.

Thus each row in the dataset details the following:

-   the number of trail users for a given day
-   trail characteristics (trail name and number, location of trailhead)
-   counter characteristics (counter ID number, owner, location along trail)
-   Strava Metro information summarized to reflect the maximum count of all edges for a given day
-   daily weather data (precipitation in inches, min/max temperature, AQI value, PM$_{2.5}$ concentration)

The data look like this:

```{r data structure, eval = T, echo=T}
head(singleTrail)
str(singleTrail)
```

The times series plot of counter and Strava counts looks like this:

```{r, eval = T}
knitr::include_graphics(path = here("output/figures/MiddleCottonwood_TS.pdf"))
```

```{r}
ggplot(singleTrail, aes(yday, count)) +
  geom_point()

ggplot(singleTrail, aes(wday, count)) +
  geom_point()

ggplot(singleTrail, aes(week, count)) +
  geom_point()

ggplot(singleTrail, aes(precipitation_in, count)) +
  geom_point()

ggplot(singleTrail, aes(temp_min_f, count)) +
  geom_point()

ggplot(singleTrail, aes(temp_max_f, count)) +
  geom_point()

ggplot(singleTrail, aes(daily_aqi_value, count)) +
  geom_point()


```

## Model Options

The following provides a very brief overview of several different types of regression models. This material is by no means an exhaustive coverage of the topic, but serves as an applied presentation allowing for us to build upon familiar modelling approaches towards what might be newer material.

### Linear Regression Model

If we start simply and try to predict trail use count as our numeric response variable ($y$) via multiple linear regression (`lm`) using predictor variables, our model would have this general form:

$$
y = b_0 + b_1x_1 + b_2x_2 + \dots + \epsilon
$$

Translated in to `R` syntax, this model would look like:

```{r lm , echo=TRUE, eval=T}
lm_mod <- lm(count ~ max.count + 
               daily_aqi_value + 
               precipitation_in + 
               temp_min_f, 
             data = singleTrail)
```

This model assumes linear relationships between variables, a Gaussian (normal) distribution for the response variable, AND independent observations - **all of which are unlikely to be met with our data.**

### Generalized Linear Regression Model

To address the issue of violated normality assumption in the previous model we can try a generalized linear model (`glm`) with a Poisson distribution (log link). The trail use count response is strictly non-negative and discrete (e.g. can't have 13.5 trail users). This model, using R syntax, would have this form:

```{r glm, eval = T, echo=T}
glm_mod <- glm(count ~ max.count + 
                 daily_aqi_value + 
                 precipitation_in + 
                 temp_min_f, 
               data = singleTrail, 
               family = poisson)
```

However, this does not address the non-linear relationship between response and predictors, nor the (temporal) dependence between observations.

### Generalized Additive Models

We can address non-linearity through a simple generalized additive model (`gam`) with the general form,

$$
y = b_0 + f_1(x_1) + f_2(x_2)\dots + \epsilon
$$ and the R syntax form of:

```{r gam, eval = T, echo = T}
gam_mod <- gam(count ~ max.count + 
                 s(yday) + 
                 s(wday, bs = "cc",  k = 7) +
                 s(month, k = 3), data = singleTrail, 
               family = poisson)
```

Here, the parametric $b$ coefficients have been replaced with smooth functions, or splines. A spline is a function defined piece-wise by polynomials. Each consist of smaller basis functions, of which we may choose between several types/forms based on the data. We may also choose how many "pieces" to use by defining the number of knots ($k$) for each spline.

Spline Fun Fact: The term spline comes from the flexible devices used by shipbuilders and draftspersons to draw smooth shapes. Thanks, Wikipedia.

In the above code we fit a `gam` which is non-linear in `yday`, `wday`, and `month` but we also include `max.count` as a linear term. Note that this variable does not have the `s` prefix indicating a (spline based) smoothing term. 

We specified a single basis function for the day of week variable (`wday`). The code `bs="cc"` indicates that we are selecting a cyclic cubic regression spline, which is a penalized cubic regression spine whose ends match up. In the context of day of week we know that Sunday/Saturday would be the 'ends' that 'match' together. If we do not specify any, the default basis function is thin plate splines. They are the default smooth for `s` terms because there is a defined sense in which they are the optimal smoother of any given basis dimension/rank (Wood, 2003). One key advantage of this approach is that it avoids the knot placement problems of conventional regression spline modelling.

We have also specified the number of knots, \$k\$, for both wday and month. If we do not specify $k$ then the default is 10. In both of these cases this results in the following errors:

> Error in place.knots(x, nk) : 
 more knots than unique data values is not allowed
>
> Error in smooth.construct.tp.smooth.spec(object, dk\$data, dk\$knots) :
> A term has fewer unique covariate combinations than specified maximum degrees of freedom

(I believe the error messages are different due to differing basis functions.) For the `wday` variable we know that there's only 7 unique values (for each day of the week) and thus this is the highest value of $k$ able to be used. Similarly, these data cover 3 `month` values and so that is our selection for $k$ in this instance.

For a more in depth description of smooth terms as specified within a `gam` formula in `R` please refer to the associated Help document using the following code:

```{r smooth help, echo = T}
?mgcv::smooth.terms 
```

<!-- https://stats.stackexchange.com/questions/359568/choosing-k-in-mgcvs-gam  -->

<!-- https://stats.stackexchange.com/questions/413955/should-i-use-poisson-or-gaussian-family-in-gam -->

<!-- https://stats.stackexchange.com/questions/407429/is-there-a-maximum-number-of-independent-variables-for-generalized-additive-mode -->

<!-- https://stats.stackexchange.com/questions/455988/how-do-you-include-change-in-a-variable-over-time-across-participants-within-a-g -->

<!-- https://stats.stackexchange.com/questions/174936/gamm-choice-of-k-basis-dimension-with-temporal-correlation -->

```{r, eval = F, echo =F}
# we know that the assumptions for temporal independance aren't met so adding in more variables later. This model does well (i.e. high R-squared value) but I want to make sure we're checking our model assumptions carefully. 

# gam_mod2 <- gam(count ~  s(yday) + 
#                       s(week, k = 8) +
#                       s(wday, 
#                         bs = "cc", k = 7) +
#                       s(daily_aqi_value) +
#                    s(temp_min_f, k = 5) +
#                    s(precipitation_in, k = 5) +
#                       max.count,
#                 data = singleTrail, 
#                 family = poisson)

```

<!-- https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/ -->

### Generalized Additive Mixed Models

Recall that an assumption of the `gam` model is that observations are **conditionally** independent. Unless we specify that the data aren't independent using `gamm` instead of `gam`, the `gam` model will perform smoothness selection assuming that we have $n$ independent observations. 
We first fit a `gamm` model to the exact same covariates and we used when fitting the previous `gam` model. The `R` syntax is as follows:

```{r gamm, eval = T, echo = T, message=F, warning=F}

gamm_mod <- gamm(count ~  max.count + 
                 s(yday) + 
                 s(wday, bs = "cc", k = 7) +
                 s(month, k = 3), 
                 data = singleTrail, 
                 family = poisson)
```

However, we may also include some more explanatory variables to better reflect our understanding of this system we are modelling. The following `gamm` model has additional non-linear terms included pulling from local weather information. We have specified `k=9` for `precipiation_in` again because the default value of 10 is too high and results in an error message and the code fails to run. 

```{r, eval = T, echo= T, message=F}
gamm_mod2 <- gamm(count ~ s(yday) + 
                    s(month, k = 3) +
                    s(wday, 
                      bs = "cc", k = 7) +
                    s(daily_aqi_value) +
                    s(temp_min_f) +
                    s(precipitation_in, k = 9) +
                    max.count,
                  data = singleTrail, 
                  family = poisson)

```

### GAMM with additional temporal autocorrelation specification

If we model temporal autocorrelation through temporal terms in the model (as we did in `gamm_mod2`), then the smooth functions of temporal variables (such as `yday`, `wday`, etc) could be already accounting for the temporal structure in the data such that once we consider the model, the observations are independent. Note that it's also possible that the temporal autocorrelation is **not** captured through the terms in the model and additional complexity would be beneficial.

For example, <AR1 vs ARMA>.

<!-- https://stats.stackexchange.com/questions/316396/built-a-covariance-matrix-for-gam-using-magic-and-magic-post-proc-now-how-to-ca -->

```{r gamm AR1, eval=T, echo = T}

 ctrl <- list(niterEM = 0, 
              optimMethod="L-BFGS-B")
 
 ## AR(1)
m.AR1 <- gamm(count ~ 
                # s(yday) + 
                s(month, k = 3) +
                s(wday, 
                  bs = "cc", k = 7) +
                s(daily_aqi_value) +
                s(temp_min_f, k = 5) +
                s(precipitation_in, k = 5) +
                max.count,
              data = singleTrail, 
              family = poisson,
              correlation = corAR1(form = ~ yday),
              control = ctrl)
 
save(m.AR1, file=here("output/models/mAR1.rda"), compress='xz')


```

<!-- https://stats.stackexchange.com/questions/482944/autocorrelative-parameter-in-gam-in-r -->

<!-- https://stats.stackexchange.com/questions/341632/seasonal-data-with-gamms -->

This shows the need for examining diagnostic plots (in this case, ACF and PACF plots for identifying autocorrelation) and summaries for each model (see Section \@ref(diagnostic-practices) for more details).


### A Quick Word: Why not a Classic Time Series Analysis?

While both times series regression and GAM's can both be applied to temporal data, the two models are fundamentally different. Time series regression makes an assumption of a stationary series, while GAM deals with the trend internally with smoothing. This relaxed assumption of stationarity is especially useful <MORE DETAILS>.

<!-- https://stats.stackexchange.com/questions/381509/time-series-analysis-via-generalized-additive-models-model-assumptions-and-stat -->

<!-- https://jacolienvanrij.com/Tutorials/GAMM.html#analysis -->

## Diagnostic Practices

```{r}
summary(lm_mod)
summary(glm_mod)
summary(gam_mod)
summary(gam_mod2)
summary(gamm_mod$gam)
summary(gamm_mod2$gam)
summary(m.AR1$gam)

# gam.check(gam_mod)
# gam.check(gam_mod2)

gratia::draw(gam_mod, residuals = T)
gratia::draw(gam_mod2, residuals = T)
gratia::draw(gamm_mod, residuals = T)
gratia::draw(gamm_mod2, residuals = T)


gratia::appraise(gam_mod)
gratia::appraise(gam_mod2)
gratia::appraise(gamm_mod$gam)
gratia::appraise(gamm_mod2$gam)

```

### Identifying AR and MA using ACF and PACF Plots

<!-- https://towardsdatascience.com/identifying-ar-and-ma-terms-using-acf-and-pacf-plots-in-time-series-forecasting-ccb9fd073db8 -->

<!-- https://cran.r-project.org/web/packages/itsadug/vignettes/acf.html -->

<!-- https://jroy042.github.io/nonlinear/week4.html -->

<!-- https://stackoverflow.com/questions/56562634/adding-an-moving-average-component-in-gams-model -->

<!-- https://stats.stackexchange.com/questions/332685/modeling-autocorrelation-in-generalized-additive-models-gam-with-re-splines -->

```{r gamm diagnostics, eval=F}

layout(matrix(1:6, ncol = 2))
plot(gamm_mod2$gam, scale = 0)
layout(1)

layout(matrix(1:2, ncol = 2))
acf(resid(gamm_mod2$lme, type = "normalized"), 
    lag.max = 36, main = "ACF")
pacf(resid(gamm_mod2$lme, type = "normalized"),
     lag.max = 36, main = "pACF")
layout(1)
```

### Choosing an Appropriate Model for Trail Use in Middle Cottonwood

Often when we fit a linear regression model, we use R-squared as a way to assess how well a model fits the data.

R-squared represents the proportion of the variance in the response variable that can be explained by the predictor variables in a regression model. This number ranges from 0 to 1, with higher values indicating a better model fit. However, there is no such R-squared value for general linear models like logistic regression models and Poisson regression models. Instead, we can calculate a metric known as McFadden's R-Squared, which ranges from 0 to just under 1, with higher values indicating a better model fit.

<!-- https://www.statology.org/glm-r-squared/ -->

You shouldn't compare the AICs between objects fitted with different software. gam() is fitted via some fancy code fu in the mgcv package, whereas your gamm() fit is actually fitted via fancy code in the MASS (glmmPQL()) and then nlme (lme()) packages. It would be common for different constants to end up in the log likelihood.

```{r model-summaries, eval=TRUE}
#create table of 
summary.table <- as.data.frame(matrix(data = NA, nrow = 6, ncol = 3))
colnames(summary.table) <- c("model", "AIC", "R.sq")

summary.table$model <- c("lm_mod",
                          "glm_mod",
                          "gam_mod",
                          # "gam_mod2",
                          "gamm_mod", 
                         "gamm_mod2", 
                         "gamm_AR1")
#lm
summary.table[1, 2:3] <- c(AIC(lm_mod),
                           summary(lm_mod)$adj.r.squared)
#glm
summary.table[2, 2:3]<- c(AIC(glm_mod), 
                          with(summary(glm_mod), 1 - deviance/null.deviance)
)

#gam
summary.table[3, 2:3] <- c(gam_mod$aic, summary(gam_mod)$r.sq)

# #gam2
# summary.table[4, 2:3] <- c(gam_mod2$aic, summary(gam_mod2)$r.sq)

#gamm
summary.table[4, 3] <- c(
  # AIC(gamm_mod$lme),
                           summary(gamm_mod$gam)$r.sq)

#gamm2
summary.table[5, 3] <- c(
  # AIC(gamm_mod$lme),
                           summary(gamm_mod2$gam)$r.sq)

#gamm.AR1
summary.table[6, 3] <- c(
  # AIC(gamm_mod$lme),
                           summary(m.AR1$gam)$r.sq)



#round r.sq values
summary.table[, 3] <- round(summary.table[, 3], 2)


knitr::kable(summary.table, 
             booktabs = T,
             format = "latex")
```

```{r}
#code from https://fromthebottomoftheheap.net/2011/07/21/smoothing-temporally-correlated-data/

edf.gamm <- summary(gamm_mod$gam)$edf
edf.gamm2 <- summary(gamm_mod2$gam)$edf

plot(singleTrail$count ~ singleTrail$yday, 
     xlab = "Day of Year", ylab = "Trail Use Count", pch = as.numeric(singleTrail$wday))
# lines(Y ~ xt, lty = "dashed", lwd = 1)
# lines(fitted(lm_mod) ~ singleTrail$yday, lty = "solid", col = "darkolivegreen", lwd = 2)
# lines(fitted(glm_mod) ~ singleTrail$yday, lty = "solid", col = "midnightblue", lwd = 2)
lines(fitted(gam_mod) ~ singleTrail$yday, lty = "solid", col = "purple", lwd = 2)
lines(fitted(gamm_mod$gam) ~ singleTrail$yday, lty = "dashed", col = "red", lwd = 2)
lines(fitted(gamm_mod2$gam) ~ singleTrail$yday, lty = "solid", col = "orange", lwd = 2)
lines(fitted(m.AR1$gam) ~ singleTrail$yday, lty = "solid" , col = "darkolivegreen", lwd = 2)

# legend("topleft",
#        legend = c("Truth",
#        paste("Cubic spline (edf = ", round(m1$df, 2), ")", sep = ""),
#        paste("AM (edf = ", round(edf2, 2), ")", sep = ""),
#        paste("AM + AR(1) (edf = ", round(edf3, 2), ")", sep = "")),
#        col = c("black", "darkgreen", "red", "midnightblue"),
#        lty = c("dashed", rep("solid", 3)),
#        lwd = c(1, rep(2, 3)),
#        bty = "n", cex = 0.8)
```

<!-- https://stackoverflow.com/questions/59825442/get-the-aic-or-bic-citerium-from-a-gamm-gam-and-lme-models-how-in-mgcv-and-h -->

<!-- https://stats.stackexchange.com/questions/318278/generalized-additive-models-gams-interactions-and-covariates -->

**Final Choice: Generalized Additive Mixture Model with Explanatory Variables**

## Visualization of Model

```{r, eval = T}
gratia::draw(gamm_mod2, residuals = T)
```

```{r, eval = T}
gratia::appraise(gamm_mod2$gam)

```

```{r, eval = T}

layout(matrix(1:2, ncol = 2))
acf(resid(gamm_mod2$lme, type = "normalized"), 
    lag.max = 36, main = "ACF")
pacf(resid(gamm_mod2$lme, type = "normalized"),
     lag.max = 36, main = "pACF")


layout(1)
```

```{r}
 with(singleTrail, tsDiagGamm(gamm_mod2, 
                              timevar = yday, observed = count))

```

```{r, eval = F, result = "asis", message=F, warning=F}
itsadug::gamtabs(gamm_mod2$gam, 
                 caption="Summary of Explanatory GAMM", 
                 comment=F,
                 type='latex')


```

```{=latex}
    \begin{table}[ht]
\centering
\begin{tabular}{lrrrr}
   \hline
A. parametric coefficients & Estimate & Std. Error & t-value & p-value \\ 
  (Intercept) & 4.9790 & 0.0337 & 147.6956 & $<$ 0.0001 \\ 
  max.count & 0.0096 & 0.0023 & 4.1454 & 0.0005 \\ 
   \hline
B. smooth terms & edf & Ref.df & F-value & p-value \\ 
  s(yday) & 8.4169 & 8.4169 & 13.9495 & $<$ 0.0001 \\ 
  s(month) & 1.0000 & 1.0000 & 18.8885 & 0.0003 \\ 
  s(wday) & 4.8314 & 5.0000 & 25.7923 & $<$ 0.0001 \\ 
  s(daily\_aqi\_value) & 4.6698 & 4.6698 & 10.4663 & $<$ 0.0001 \\ 
  s(temp\_min\_f) & 4.2163 & 4.2163 & 14.4487 & $<$ 0.0001 \\ 
  s(precipitation\_in) & 3.7374 & 3.7374 & 42.5743 & $<$ 0.0001 \\ 
   \hline
\end{tabular}
\caption{Summary of Explanatory GAMM} 
\label{tab.gam}
\end{table}
```
```{r}
mgcv::vis.gam(gamm_mod2$gam, 
                 view=c("precipitation_in", "temp_min_f"),
                 select=2,
                 main='Explanatory GAMM', labcex=.8,
                print.summary=FALSE)
```

<https://stats.stackexchange.com/questions/425194/visreg-visualization-of-mgcv-results-gam>

<https://cran.r-project.org/web/packages/itsadug/vignettes/inspect.html>

## Prediction and Forecasting Trail Use

```{r}
 # itsadug::get_predictions(model = gamm_mod2$gam,
 #                          cond = list(yday = seq(0, 365, length = 50)))

pred <- marginaleffects::predictions(gamm_mod2$gam)
pred.datagrid <- marginaleffects::predictions(gamm_mod2$gam,
                                         newdata = datagrid())
pred.newdata <- marginaleffects::predictions(gamm_mod2$gam,
                                         newdata = predict.data)


#  plot(count ~ yday, data = singleTrail, type = "p")
# lines(predicted ~ yday, data = pred, col = "red")
# # lines(p2 ~ yday, data = predict.data, col = "blue")
# legend("topleft",
#        legend = c("Uncorrelated Errors","AR(1) Errors"),
#        bty = "n", col = c("red","blue"), lty = 1)


marginaleffects::plot_cap(gamm_mod2$gam, condition = "yday") #+    
  # ggplot2::xlim(100, 305) + 
  # ylim(0, 1000) +
  # geom_line(data = pred.newdata, 
            # aes(x = yday, y = predicted), 
            # color = 'red', size = 1) 



marginaleffects::plot_cap(gamm_mod2$gam, condition = "max.count")
marginaleffects::plot_cap(gamm_mod2$gam, condition = "wday")




```

<!-- # Speculations on Applying <CHOSEN> Model to All Trails -->

# Useful links

<!-- overview of multiple models leading up to gamm -->

<https://m-clark.github.io/generalized-additive-models/application.html>

<!-- simple overview of splines and basis functions -->

<https://asbates.rbind.io/2019/02/04/what-are-splines/>
