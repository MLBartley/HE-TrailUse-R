
# Model Choice Explained with Application to Middle Cottonwood Trail

```{r, echo=FALSE, message=F, warning=F}
library(dplyr)
library(here)
library(ggplot2)
library(gratia)
library(itsadug)
library(lubridate)
library(magrittr)
library(marginaleffects)
library(mgcv)
library(xtable)

source(here("scripts/01-LoadData.R"))

#helps to format this document 
#by default the included code chunks do not appear in the report view
knitr::opts_chunk$set(echo = F, 
                      eval = F,
                      message = F, 
                      cache=TRUE, 
                      warning=FALSE, 
                      error=FALSE, 
                      root.dir = normalizePath(".."))

```

```{r, eval = T}
## Model Checking function
tsDiagGamm <- function(x, timevar, observed, f = 0.3, type = "normalized") {
  resi <- resid(x$lme, type = type)
  fits <- fitted(x$lme)
  on.exit(layout(1))
  layout(matrix(1:6, ncol = 3, byrow = TRUE))
  plot(resi ~ fits, ylab = "Normalized Residuals",
       xlab = "Fitted Values", main = "Fitted vs. Residuals")
  lines(lowess(x = fits, y = resi, f = f), col = "blue",
        lwd = 2)
  plot(resi ~ timevar, ylab = "Normalized Residuals",
       xlab = "Time", main = "Time series of residuals")
  lines(lowess(x = timevar, y = resi, f = f), col = "blue", lwd = 2)
  plot(observed ~ fits, ylab = "Observed",
       xlab = "Fitted Values", main = "Fitted vs. Observed",
       type = "n")
  abline(a = 0, b = 1, col = "red")
  points(observed ~ fits)
  lines(lowess(x = fits, y = observed, f = f), col = "blue",
        lwd = 2)
  hist(resi, freq = FALSE, xlab = "Normalized Residuals")
  qqnorm(resi)
  qqline(resi)
  acf(resi, main = "ACF of Residuals")
}
```


The aim of this project is to understand and predict the pattern of trail use recorded at multiple multi-use trails in the Bridger Mountains. We want to be able to predict trail use over a full calendar year and in locations where no trail counter data exist, within the Bridgers and nearby trails. In this report we start off with a simple `gam` model and explore gradually more complicated versions which we apply to a single trail (rather than then entire network of trails) so we can focus on understanding the modelling aspect for now.

We being with a quick look at the data available for our chosen trail, Middle Cottonwood Trail. Then we will progress through several `ga(m)m` models that incorporate more complexity with each iteration. This will include a detailed overview of useful diagnostic tools, a look at what explanatory predictor variables should/could be included, as well as an example of how we may forecast (predict into the near future) using our chosen model.

## The Data

```{r combine-data , eval = T}
## Focusing on Middle Cottenwood Trail (Trail # 586, Counter ID # 31)

singleCount <- trail_count %>%
  dplyr::filter(counterid == 31)

singleStrava <- strava_day %>%
  dplyr::filter(trailnumber == 586) %>% 
  # we want a single representation count value for multiple edges per day
  dplyr::group_by(timeframe) %>% 
  dplyr::mutate(max.count = max(totaltrips)) %>% 
  dplyr::select(-c(totaltrips, totalpeople, edge_uid)) %>% 
  dplyr::distinct()
# trail characteristics not pertinent for single trail analysis
# no variation within trail information

# no variation within trail information

## Combine Camera Counts, Strava Counts, Weather, Trail Characteristics

singleTrail <- singleCount %>%
  dplyr::left_join(
    singleStrava,
    by = c(
      "date" = "timeframe",
      "trailnumber" = "trailnumber",
      "trailname" = "trailname",
      "wday" = "wday"
    )
  ) %>%
  dplyr::left_join(weather, by = c("date" = "date"))

## Add in a Time variable
singleTrail$yday <- lubridate::yday(singleTrail$date)

## Assume missing Strava data reflect a 0 observation

singleTrail$max.count[which(is.na(singleTrail$max.count))] <- 0

## remove unecessary columns
singleTrail <- singleTrail %>% 
  dplyr::select(-c("counterid",
                   "trailnumber",
                   # "subsectionname", 
                   "counterowner",
                   "th_lat",
                   "th_long",
                   "counter_lat", 
                   "counter_long",
                   "stravaedgeid2022", 
                  "Segment with trail counter"                   
  )) %>% 
  dplyr::distinct()

#convert week and wday to integers now
singleTrail$wday <- as.integer(singleTrail$wday)
singleTrail$week <- as.integer(singleTrail$week)


## remove rows with NA in temp_min_F (only NA in covariates used in model fitting)
# singleTrail <- singleTrail[-49, ]


## gather strava+covariate data w/out camera counter data for predicting

predict.data <- singleStrava %>% 
  dplyr::left_join(weather, by = c("timeframe" = "date")) %>% 
  dplyr::filter(timeframe %notin% singleCount$date) %>% 
  dplyr::mutate(yday = lubridate::yday(timeframe)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(c("yday", 
                  "wday", 
                  "max.count", 
                  "month", 
                  "precipitation_in", 
                  "temp_min_f", 
                  "daily_aqi_value"))

#convert week and wday to integers now
predict.data$wday <- as.integer(predict.data$wday)
# predict.data$week <- as.integer(predict.data$week)


```

This 2.3-mile out-and-back trail was monitored via infrared counters between `r min(singleTrail$date)` and `r max(singleTrail$date)`. This daily time series count data is combined with data from Strava Metro, and local weather data.

Strava Metro partners with Headwaters Economics to provide aggregated (daily, monthly, or annually) trail use information for the Bridgers. These data are broken into multiple edges within a single trail. Eight such edges are included for Middle Cottonwood. These edges do not represent independent observations as they represent trail use on connected locations and thus likely reflect the same trail users continuing along the trail. We summarize these data to a single value using the edge ID with the maximum observed number of trail numbers for a given day.

Thus each row in the dataset details the following:

-   the number of trail users for a given day
-   trail characteristics (trail name and number, location of trailhead)
-   counter characteristics (counter ID number, owner, location along trail)
-   Strava Metro information summarized to reflect the maximum count of all edges for a given day
-   daily weather data (precipitation in inches, min/max temperature, AQI value, PM$_{2.5}$ concentration)

The data look like this:

```{r data structure, eval = T, echo=T}
head(singleTrail)
str(singleTrail)
summary(singleTrail)
```

The times series plot of counter and Strava counts looks like this:

```{r, eval = T}
knitr::include_graphics(path = here("output/figures/MiddleCottonwood_TS.pdf"))
```

```{r}
ggplot(singleTrail, aes(yday, count)) +
  geom_point()

ggplot(singleTrail, aes(wday, count)) +
  geom_point()

ggplot(singleTrail, aes(week, count)) +
  geom_point()

ggplot(singleTrail, aes(precipitation_in, count)) +
  geom_point()

ggplot(singleTrail, aes(temp_min_f, count)) +
  geom_point()

ggplot(singleTrail, aes(temp_max_f, count)) +
  geom_point()

ggplot(singleTrail, aes(daily_aqi_value, count)) +
  geom_point()


```

## Model Options

The following provides a very brief overview of several different types of regression models. This material is by no means an exhaustive coverage of the topic, but serves as an applied presentation allowing for us to build upon familiar modelling approaches towards what might be newer material.

### Linear Regression Model

If we start simply and try to predict trail use count as our numeric response variable ($y$) via multiple linear regression (`lm`) using predictor variables, our model would have this general form:

$$
y = b_0 + b_1x_1 + b_2x_2 + \dots + \epsilon
$$

Translated in to `R` syntax, this model would look like:

```{r lm , echo=TRUE, eval=T}
lm_mod <- lm(count ~ max.count + 
               daily_aqi_value + 
               precipitation_in + 
               temp_min_f, 
             data = singleTrail)
```

This model assumes linear relationships between variables, a Gaussian (normal) distribution for the response variable, AND independent observations - **all of which are unlikely to be met with our data.**

### Generalized Linear Regression Model

To address the issue of violated normality assumption in the previous model we can try a generalized linear model (`glm`) with a Poisson distribution (log link). The trail use count response is strictly non-negative and discrete (e.g. can't have 13.5 trail users). This model, using R syntax, would have this form:

```{r glm, eval = T, echo=T}
glm_mod <- glm(count ~ max.count + 
                 daily_aqi_value + 
                 precipitation_in + 
                 temp_min_f, 
               data = singleTrail, 
               family = poisson)
```

However, this does not address the non-linear relationship between response and predictors, nor the (temporal) dependence between observations.

### Generalized Additive Models

We can address non-linearity through a simple generalized additive model (`gam`) with the general form,

$$
y = b_0 + f_1(x_1) + f_2(x_2)\dots + \epsilon
$$ and the R syntax form of:

```{r gam, eval = T, echo = T}
gam_mod <- gam(count ~ max.count + 
                 s(yday) + 
                 s(wday, bs = "cc",  k = 7) +
                 s(month, k = 3), data = singleTrail, 
               family = poisson)
```

Here, the parametric $b$ coefficients have been replaced with smooth functions, or splines. A spline is a function defined piece-wise by polynomials. Each consist of smaller basis functions, of which we may choose between several types/forms based on the data. We may also choose how many "pieces" to use by defining the number of knots ($k$) for each spline.

Spline Fun Fact: The term spline comes from the flexible devices used by shipbuilders and draftspersons to draw smooth shapes. Thanks, Wikipedia.

To model a potentially nonlinear smooth or surface, three different smooth functions are available:

 * s() : for modeling a 1-dimensional smooth, or
for modeling isotropic interactions (variables are measured in same units and on same scale)
 * te(): for modeling 2- or n-dimensional interaction surfaces of variables that are not isotropic (but see info about d parameter below). Includes ‘main’ effects.
 * ti(): for modeling 2- or n-dimensional interaction surfaces that do not include the ‘main effects’.

In the above code we fit a `gam` which is non-linear in `yday`, `wday`, and `month` but we also include `max.count` as a linear term. Note that this variable does not have the `s` prefix indicating a (spline based) smoothing term. 

We specified a single basis function for the day of week variable (`wday`). The code `bs="cc"` indicates that we are selecting a cyclic cubic regression spline, which is a penalized cubic regression spine whose ends match up. In the context of day of week we know that Sunday/Saturday would be the 'ends' that 'match' together. If we do not specify any, the default basis function is thin plate splines. They are the default smooth for `s` terms because there is a defined sense in which they are the optimal smoother of any given basis dimension/rank (Wood, 2003). One key advantage of this approach is that it avoids the knot placement problems of conventional regression spline modelling.

We have also specified the number of knots, \$k\$, for both wday and month. If we do not specify $k$ then the default is 10. In both of these cases this results in the following errors:

> Error in place.knots(x, nk) : 
 more knots than unique data values is not allowed
>
> Error in smooth.construct.tp.smooth.spec(object, dk\$data, dk\$knots) :
> A term has fewer unique covariate combinations than specified maximum degrees of freedom

(I believe the error messages are different due to differing basis functions.) For the `wday` variable we know that there's only 7 unique values (for each day of the week) and thus this is the highest value of $k$ able to be used. Similarly, these data cover 3 `month` values and so that is our selection for $k$ in this instance.

For a more in depth description of smooth terms as specified within a `gam` formula in `R` please refer to the associated Help document using the following code:

```{r smooth help, echo = T}
?mgcv::smooth.terms 
```

<!-- https://stats.stackexchange.com/questions/359568/choosing-k-in-mgcvs-gam  -->

<!-- https://stats.stackexchange.com/questions/413955/should-i-use-poisson-or-gaussian-family-in-gam -->

<!-- https://stats.stackexchange.com/questions/407429/is-there-a-maximum-number-of-independent-variables-for-generalized-additive-mode -->

<!-- https://stats.stackexchange.com/questions/455988/how-do-you-include-change-in-a-variable-over-time-across-participants-within-a-g -->

<!-- https://stats.stackexchange.com/questions/174936/gamm-choice-of-k-basis-dimension-with-temporal-correlation -->

```{r, eval = F, echo =F}
# we know that the assumptions for temporal independance aren't met so adding in more variables later. This model does well (i.e. high R-squared value) but I want to make sure we're checking our model assumptions carefully. 

# gam_mod2 <- gam(count ~  s(yday) + 
#                       s(week, k = 8) +
#                       s(wday, 
#                         bs = "cc", k = 7) +
#                       s(daily_aqi_value) +
#                    s(temp_min_f, k = 5) +
#                    s(precipitation_in, k = 5) +
#                       max.count,
#                 data = singleTrail, 
#                 family = poisson)

```

<!-- https://petolau.github.io/Analyzing-double-seasonal-time-series-with-GAM-in-R/ -->

### Generalized Additive Mixed Models

Recall that an assumption of the `gam` model is that observations are **conditionally** independent. Unless we specify that the data aren't independent using `gamm` instead of `gam`, the `gam` model will perform smoothness selection assuming that we have $n$ independent observations. 
We first fit a `gamm` model to the exact same covariates and we used when fitting the previous `gam` model. The `R` syntax is as follows:

```{r gamm, eval = T, echo = T, message=F, warning=F}

gamm_mod <- gamm(count ~  max.count + 
                 s(yday) + 
                 s(wday, bs = "cc", k = 7) +
                 s(month, k = 3), 
                 data = singleTrail, 
                 family = poisson)
```

However, we may also include some more explanatory variables to better reflect our understanding of this system we are modelling. The following `gamm` model has additional non-linear terms included pulling from local weather information. We have specified `k=9` for `precipiation_in` again because the default value of 10 is too high and results in an error message and the code fails to run. 

```{r, eval = T, echo= T, message=F}
gamm_mod2 <- gamm(count ~ s(yday) + 
                    s(month, k = 3) +
                    s(wday, 
                      bs = "cc", k = 7) +
                    s(daily_aqi_value) +
                    s(temp_min_f) +
                    s(precipitation_in, k = 9) +
                    max.count,
                  data = singleTrail, 
                  family = poisson)

```

```{r}
gamm_mod2_nb <- gamm(count ~ s(yday) + 
                    s(month, k = 3) +
                    s(wday, 
                      bs = "cc", k = 7) +
                    s(daily_aqi_value) +
                    s(temp_min_f) +
                    s(precipitation_in, k = 9) +
                    max.count,
                  data = singleTrail, 
                  family = negbin(1))


gamm_mod2_gaus <- gamm(count ~ s(yday) + 
                    s(month, k = 3) +
                    s(wday, 
                      bs = "cc", k = 7) +
                    s(daily_aqi_value) +
                    s(temp_min_f) +
                    s(precipitation_in, k = 9) +
                    max.count,
                  data = singleTrail)
```


### GAMM with additional temporal autocorrelation specification

If we model temporal autocorrelation through temporal terms in the model (as we did in `gamm_mod2`), then the smooth functions of temporal variables (such as `yday`, `wday`, etc) could be already accounting for the temporal structure in the data such that once we consider the model, the observations are independent. Note that it's also possible that the temporal autocorrelation is **not** captured through the terms in the model and additional complexity would be beneficial.

For example, <AR1 vs ARMA>.

<!-- https://stats.stackexchange.com/questions/316396/built-a-covariance-matrix-for-gam-using-magic-and-magic-post-proc-now-how-to-ca -->

```{r gamm AR1, eval=T, echo = T}

 ctrl <- list(niterEM = 0, 
              optimMethod="L-BFGS-B")
 
 ## AR(1)
m.AR1 <- gamm(count ~ 
                # s(yday) + 
                s(month, k = 3) +
                s(wday, 
                  bs = "cc", k = 7) +
                s(daily_aqi_value) +
                s(temp_min_f, k = 5) +
                s(precipitation_in, k = 5) +
                max.count,
              data = singleTrail, 
              family = poisson,
              correlation = corAR1(form = ~ yday),
              control = ctrl)
 
save(m.AR1, file=here("output/models/mAR1.rda"), compress='xz')


```

<!-- https://stats.stackexchange.com/questions/482944/autocorrelative-parameter-in-gam-in-r -->

<!-- https://stats.stackexchange.com/questions/341632/seasonal-data-with-gamms -->

This shows the need for examining diagnostic plots (in this case, ACF and PACF plots for identifying autocorrelation) and summaries for each model (see Section \@ref(diagnostic-practices) for more details).


### A Quick Word: Why not a Classic Time Series Analysis?

While both times series regression and GAM's can both be applied to temporal data, the two models are fundamentally different. Time series regression makes an assumption of a stationary series, while GAM deals with the trend internally with smoothing. This relaxed assumption of stationarity is especially useful <MORE DETAILS>.

<!-- https://stats.stackexchange.com/questions/381509/time-series-analysis-via-generalized-additive-models-model-assumptions-and-stat -->

<!-- https://jacolienvanrij.com/Tutorials/GAMM.html#analysis -->

## Diagnostic Practices

Once a model (or a suite of models) is fit, we need to take a more detailed look at model outputs to learn how to interpret the results of our model-fitting and better understand the relationships between variables.

### Summarize

We start with the `summary()` function which may be applied to a `gam` object in R. Below we have the summary output for our `GAMM` model with explanatory variables.

```{r, summary(), eval = T}
# summary(lm_mod)
# summary(glm_mod)
# summary(gam_mod)
# summary(gam_mod2)
# summary(gamm_mod$gam)
summary(gamm_mod2$gam)
# summary(m.AR1$gam)
```
The first part of the summary describes the model we fit. The "Family" component tells us the model assumes a Poisson distribution of our response, and the "Link" of "log" shows that the model transforms the predictions. 

The next section describes the parametric terms of our model, referring to the linear terms in the model. This section may be familiar from linear modeling. It shows the coefficients for the linear terms in the model, their values, errors, test statistics, and p-values. Asterisks next to the p-values indicate statistical significance. In this case, the model intercept is significant, and the fixed effect of max.count (the number of aggregated/binned Strava trip counts) is also significant at the 0 level.

The next section covers smooth terms. For these smooths the summary for coefficients is not printed. This is because each smooth has coefficients for each basis function. Instead, the first column reads edf, which stands for effective degrees of freedom. This value represents the complexity of the smooth. An edf of 1 is equivalent to a straight line. An edf of 2 is equivalent to a quadratic curve, and so on, with higher edfs describing more wiggly curves. In our summary, all included smooth terms are significant at the 0.001 level. Note that the 'month' term has an edf of 1, indicating the smooth is equivalent to a straight line. The day of year term ('yday') has the highest edf (8.338) and thus highest wiggliness. [COMPARE TO PARTIAL EFFECT PLOTS]

The terms to the right of the EDF column have to do with significance testing for smooths. The Ref.df and F columns are test statistics used in an ANOVA test to test overall significance of the smooth. The result of this test is the p-value to the right. It's important to note that these values are approximate, and it's important to visualize your model to check them.

The R-sq(adj.) does not a straight-forward interpretation of a measure of "proportion of variance explained" in nonlinear regression, and thus can not be employed as an absolute measure of model performance. Deviance explained should be a more generalized measurement of goodness of fit especially for non-gaussian models (such as we have here).

The scale estimate is $\hat{\phi}$ , i.e. this is the value of $\phi$ estimated during model fitting. For the Poisson and Binomial families/distributions, by definition $\phi$=1, but for other distributions this is not the case, including the Gaussian. In the Gaussian case, $\hat{\phi}$  is the residual standard error squared.

### Visualize

After examining the summary, we should visualize our results. This may be accomplished using the `plot()` function, however we choose to use the `gratia` package and it's `draw()` function to produce these plots in ggplot2 rather than base R. 

```{r draw(), eval=T, echo=T}
# plot(gamm_mod2$gam)

# gratia::draw(gam_mod, residuals = T)
# gratia::draw(gam_mod2, residuals = T)
# gratia::draw(gamm_mod, residuals = T)
gratia::draw(gamm_mod2, residuals = T)
```

The plots generated by gratias draw()  (or mgcv's plot()) function are partial effect plots. That is, they show the component effect of each of the smooth or linear terms in the model, which add up to the overall prediction. We often want to show data alongside model predictions. These plots aid in this by (1) including covariate values along the bottom axis of the plots and (2) plotting partial residuals (here in light blue) on the plots. Partial residuals are the difference between the partial effect and the data, after all other partial effects have been accounted for. These plots also include shading representing 95% confidence interval for the mean shape of the effect.

### GAM Check

[add lots of GAMM caveats]

Running gam.check() on a model provides several outputs, in both the console and as plots. We'll start with the console output.

First, gam.check() reports on model convergence. Here, it reports full convergence. R has found a best solution. If the model has not converged, results are likely not correct. This can happen when there are too many parameters in the model for not enough data.

Below, we see a table of basis checking results. This shows a statistical test for patterns in model residuals, which should be random. Each line reports the test results for one smooth. It shows the k value or number of basis functions, the effective degrees of freedom, a test statistic, and p-value.

Here, small p-values indicate that residuals are not randomly distributed. This often means there are not enough basis functions.

This is an approximate test. Always visualize your results too, and compare the k and edf values in addition to looking at the p-value.

```{r, eval = T, echo = T}
# gam.check(gam_mod)
# gam.check(gam_mod2)
gam.check(gamm_mod2$gam,)
```

Each of these gives a different way of looking at your model residuals. These plots show the results from the original, poorly fit model. On the top-left is a Q-Q plot, which compares the model residuals to a normal distribution. A well-fit model's residuals will be close to a straight line. On bottom left is a histogram of residuals. We would expect this to have a symmetrical bell shape. On top-right is a plot of residual values. These should be evenly distributed around zero. Finally, on the bottom-right is plot of response against fitted values. A perfect model would form a straight line. We don't expect a perfect model, but we do expect the pattern to cluster around the 1-to-1 line.

```{r, echo = T, eval = T}
# gratia::appraise(gam_mod)
# gratia::appraise(gam_mod2)
# gratia::appraise(gamm_mod$gam)
gratia::appraise(gamm_mod2$gam)
```

Fitted vs. response plots are generally less useful for non-normally distributed data as it can be difficult to visually assess if the observed data shows more heteroskedasticity than expected.

```{r tsDiagGAMM, eval = T, echo= T}

singleTrail_dropna <- singleTrail %>% 
  tidyr::drop_na('daily_aqi_value')

with(singleTrail_dropna, 
     tsDiagGamm(gamm_mod2, 
                timevar = yday,
                observed = count))
```


### Rootogram

A good way to check how well the model compares with the observed data (and hence check for over-dispersion in the data relative to the conditional distribution implied by the model) is via a rootogram. A rootogram is a model diagnostic tool that assesses the goodness of fit of a statistical model. The observed values of the response are compared with those expected from the fitted model. For discrete, count responses, the frequency of each count (0, 1, 2, etc) in the observed data and expected from the conditional distribution of the response implied by the model are compared.

```{r, echo = T, eval = T}
rg <- gratia::rootogram(gamm_mod2$gam)
draw(rg)
```

Looking at [FIGURE] we see the main features of the rootogram:

* expected counts, given the model, are shown by the thick blue line,
* observed counts are shown as bars, which in a hanging rootogram are show hanging from the blue line of expected counts,
* on the x-axis we have the count bin, 0 count, 1 count, 2 count, etc,
* on the y-axis we have the square root of the observed or expected count — the square root transformation allows for departures from expectations to be seen even at small frequencies,
* a reference line is drawn at a height of 0,
* and     

Because this is a hanging rootogram, we can think of the rootogram as relating to the fitted counts — if a bar doesn’t reach the zero line then the model over predicts a particular count bin, and if the bar exceeds the zero line it under predicts.

```{r}
sum(residuals(gamm_mod2$gam, type = "pearson")^2) / df.residual(gamm_mod2$gam)


```

### Identifying AR and MA using ACF and PACF Plots

<!-- https://towardsdatascience.com/identifying-ar-and-ma-terms-using-acf-and-pacf-plots-in-time-series-forecasting-ccb9fd073db8 -->

<!-- https://cran.r-project.org/web/packages/itsadug/vignettes/acf.html -->

<!-- https://jroy042.github.io/nonlinear/week4.html -->

<!-- https://stackoverflow.com/questions/56562634/adding-an-moving-average-component-in-gams-model -->

<!-- https://stats.stackexchange.com/questions/332685/modeling-autocorrelation-in-generalized-additive-models-gam-with-re-splines -->

```{r gamm diagnostics, eval=F}

layout(matrix(1:6, ncol = 2))
plot(gamm_mod2$gam, scale = 0)
layout(1)

layout(matrix(1:2, ncol = 2))
acf(resid(gamm_mod2$lme, type = "normalized"), 
    lag.max = 36, main = "ACF")
pacf(resid(gamm_mod2$lme, type = "normalized"),
     lag.max = 36, main = "pACF")
layout(1)
```

### Choosing an Appropriate Model for Trail Use in Middle Cottonwood

Often when we fit a linear regression model, we use R-squared as a way to assess how well a model fits the data.

R-squared represents the proportion of the variance in the response variable that can be explained by the predictor variables in a regression model. This number ranges from 0 to 1, with higher values indicating a better model fit. However, there is no such R-squared value for general linear models like logistic regression models and Poisson regression models. Instead, we can calculate a metric known as McFadden's R-Squared, which ranges from 0 to just under 1, with higher values indicating a better model fit.

<!-- https://www.statology.org/glm-r-squared/ -->

You shouldn't compare the AICs between objects fitted with different software. gam() is fitted via some fancy code fu in the mgcv package, whereas your gamm() fit is actually fitted via fancy code in the MASS (glmmPQL()) and then nlme (lme()) packages. It would be common for different constants to end up in the log likelihood.

```{r model-summaries, eval=TRUE}
#create table of 
summary.table <- as.data.frame(matrix(data = NA, nrow = 6, ncol = 3))
colnames(summary.table) <- c("model", "AIC", "R.sq")

summary.table$model <- c("lm_mod",
                          "glm_mod",
                          "gam_mod",
                          # "gam_mod2",
                          "gamm_mod", 
                         "gamm_mod2", 
                         "gamm_AR1"
                         )
#lm
summary.table[1, 2:3] <- c(AIC(lm_mod),
                           summary(lm_mod)$adj.r.squared)
#glm
summary.table[2, 2:3]<- c(AIC(glm_mod), 
                          with(summary(glm_mod), 1 - deviance/null.deviance)
)

#gam
summary.table[3, 2:3] <- c(gam_mod$aic, summary(gam_mod)$r.sq)

# #gam2
# summary.table[4, 2:3] <- c(gam_mod2$aic, summary(gam_mod2)$r.sq)

#gamm
summary.table[4, 3] <- c(
  # AIC(gamm_mod$lme),
                           summary(gamm_mod$gam)$r.sq)

#gamm2
summary.table[5, 3] <- c(
  # AIC(gamm_mod$lme),
                           summary(gamm_mod2$gam)$r.sq)

#gamm.AR1
summary.table[6, 3] <- c(
  # AIC(gamm_mod$lme),
                           summary(m.AR1$gam)$r.sq)



#round r.sq values
summary.table[, 3] <- round(summary.table[, 3], 2)


knitr::kable(summary.table, 
             booktabs = T,
             format = "latex")
```

```{r}
#code from https://fromthebottomoftheheap.net/2011/07/21/smoothing-temporally-correlated-data/

edf.gamm <- summary(gamm_mod$gam)$edf
edf.gamm2 <- summary(gamm_mod2$gam)$edf

plot(singleTrail$count ~ singleTrail$yday, 
     xlab = "Day of Year", ylab = "Trail Use Count", pch = as.numeric(singleTrail$wday))
# lines(Y ~ xt, lty = "dashed", lwd = 1)
# lines(fitted(lm_mod) ~ singleTrail$yday, lty = "solid", col = "darkolivegreen", lwd = 2)
# lines(fitted(glm_mod) ~ singleTrail$yday, lty = "solid", col = "midnightblue", lwd = 2)
lines(fitted(gam_mod) ~ singleTrail$yday, lty = "solid", col = "purple", lwd = 2)
lines(fitted(gamm_mod$gam) ~ singleTrail$yday, lty = "dashed", col = "red", lwd = 2)
lines(fitted(gamm_mod2$gam) ~ singleTrail$yday, lty = "solid", col = "orange", lwd = 2)
lines(fitted(m.AR1$gam) ~ singleTrail$yday, lty = "solid" , col = "darkolivegreen", lwd = 2)

# legend("topleft",
#        legend = c("Truth",
#        paste("Cubic spline (edf = ", round(m1$df, 2), ")", sep = ""),
#        paste("AM (edf = ", round(edf2, 2), ")", sep = ""),
#        paste("AM + AR(1) (edf = ", round(edf3, 2), ")", sep = "")),
#        col = c("black", "darkgreen", "red", "midnightblue"),
#        lty = c("dashed", rep("solid", 3)),
#        lwd = c(1, rep(2, 3)),
#        bty = "n", cex = 0.8)
```

<!-- https://stackoverflow.com/questions/59825442/get-the-aic-or-bic-citerium-from-a-gamm-gam-and-lme-models-how-in-mgcv-and-h -->

<!-- https://stats.stackexchange.com/questions/318278/generalized-additive-models-gams-interactions-and-covariates -->

**Final Choice: Generalized Additive Mixture Model with Explanatory Variables**

<!-- ## Visualization of Model -->

<!-- ```{r, eval = T} -->
<!-- gratia::draw(gamm_mod2, residuals = T) -->
<!-- ``` -->

<!-- ```{r, eval = T} -->
<!-- gratia::appraise(gamm_mod2$gam) -->

<!-- ``` -->

<!-- ```{r, eval = T} -->

<!-- layout(matrix(1:2, ncol = 2)) -->
<!-- acf(resid(gamm_mod2$lme, type = "normalized"),  -->
<!--     lag.max = 36, main = "ACF") -->
<!-- pacf(resid(gamm_mod2$lme, type = "normalized"), -->
<!--      lag.max = 36, main = "pACF") -->


<!-- layout(1) -->
<!-- ``` -->

<!-- ```{r} -->
<!--  with(singleTrail, tsDiagGamm(gamm_mod2,  -->
<!--                               timevar = yday, observed = count)) -->

<!-- ``` -->

<!-- ```{r, eval = F, result = "asis", message=F, warning=F} -->
<!-- itsadug::gamtabs(gamm_mod2$gam,  -->
<!--                  caption="Summary of Explanatory GAMM",  -->
<!--                  comment=F, -->
<!--                  type='latex') -->


<!-- ``` -->

<!-- ```{=latex} -->
<!--     \begin{table}[ht] -->
<!-- \centering -->
<!-- \begin{tabular}{lrrrr} -->
<!--    \hline -->
<!-- A. parametric coefficients & Estimate & Std. Error & t-value & p-value \\  -->
<!--   (Intercept) & 4.9790 & 0.0337 & 147.6956 & $<$ 0.0001 \\  -->
<!--   max.count & 0.0096 & 0.0023 & 4.1454 & 0.0005 \\  -->
<!--    \hline -->
<!-- B. smooth terms & edf & Ref.df & F-value & p-value \\  -->
<!--   s(yday) & 8.4169 & 8.4169 & 13.9495 & $<$ 0.0001 \\  -->
<!--   s(month) & 1.0000 & 1.0000 & 18.8885 & 0.0003 \\  -->
<!--   s(wday) & 4.8314 & 5.0000 & 25.7923 & $<$ 0.0001 \\  -->
<!--   s(daily\_aqi\_value) & 4.6698 & 4.6698 & 10.4663 & $<$ 0.0001 \\  -->
<!--   s(temp\_min\_f) & 4.2163 & 4.2163 & 14.4487 & $<$ 0.0001 \\  -->
<!--   s(precipitation\_in) & 3.7374 & 3.7374 & 42.5743 & $<$ 0.0001 \\  -->
<!--    \hline -->
<!-- \end{tabular} -->
<!-- \caption{Summary of Explanatory GAMM}  -->
<!-- \label{tab.gam} -->
<!-- \end{table} -->
<!-- ``` -->
<!-- ```{r} -->
<!-- mgcv::vis.gam(gamm_mod2$gam,  -->
<!--                  view=c("precipitation_in", "temp_min_f"), -->
<!--                  select=2, -->
<!--                  main='Explanatory GAMM', labcex=.8, -->
<!--                 print.summary=FALSE) -->
<!-- ``` -->

<!-- <https://stats.stackexchange.com/questions/425194/visreg-visualization-of-mgcv-results-gam> -->

<!-- <https://cran.r-project.org/web/packages/itsadug/vignettes/inspect.html> -->

## Prediction and Forecasting Trail Use

```{r}
 # itsadug::get_predictions(model = gamm_mod2$gam,
 #                          cond = list(yday = seq(0, 365, length = 50)))

pred <- marginaleffects::predictions(gamm_mod2$gam)
pred.datagrid <- marginaleffects::predictions(gamm_mod2$gam,
                                         newdata = datagrid())
pred.newdata <- marginaleffects::predictions(gamm_mod2$gam,
                                         newdata = predict.data)


#  plot(count ~ yday, data = singleTrail, type = "p")
# lines(predicted ~ yday, data = pred, col = "red")
# # lines(p2 ~ yday, data = predict.data, col = "blue")
# legend("topleft",
#        legend = c("Uncorrelated Errors","AR(1) Errors"),
#        bty = "n", col = c("red","blue"), lty = 1)


marginaleffects::plot_cap(gamm_mod2$gam, condition = "yday") #+    
  # ggplot2::xlim(100, 305) + 
  # ylim(0, 1000) +
  # geom_line(data = pred.newdata, 
            # aes(x = yday, y = predicted), 
            # color = 'red', size = 1) 



marginaleffects::plot_cap(gamm_mod2$gam, condition = "max.count")
marginaleffects::plot_cap(gamm_mod2$gam, condition = "wday")




```

<!-- # Speculations on Applying <CHOSEN> Model to All Trails -->

<!-- ## Useful links -->

<!-- <!-- overview of multiple models leading up to gamm --> -->

<!-- <https://m-clark.github.io/generalized-additive-models/application.html> -->

<!-- <!-- simple overview of splines and basis functions --> -->

<!-- <https://asbates.rbind.io/2019/02/04/what-are-splines/> -->
