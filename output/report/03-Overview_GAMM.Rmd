# Explanation of Models {#Models}
```{r, echo=FALSE, message=F, warning=F}
library(dplyr)
library(here)
library(lubridate)
library(ggplot2)
library(magrittr)
library(mgcv)

source(here("scripts/01-LoadData.R"))

#helps to format this document 
#by default the included code chunks do not appear in the report view
knitr::opts_chunk$set(echo = F, 
                      eval = F,
                      message = F, 
                      # cache=TRUE, 
                      warning=FALSE, 
                      error=FALSE, 
                      root.dir = normalizePath(".."))

```
The aim of this project is to understand and predict the pattern of trail use recorded at multiple multi-use trails in the Bridger Mountains. We want to be able to predict trail use over a full calendar year and in locations where no trail counter data exist, within the Bridgers and nearby trails. Here, we provide a description of the statistical approach employed (generalized additive mixture modeling) in addition to brief overviews of modelling approaches that `gamm`s build upon (e.g. linear regression, generalized linear regression, and generalized additive models). In Section \@ref(MidCot), we further this exploration in model choice by applying each to a single trail, Middle Cottonwood. 

## Linear Regression Model

Linear regression is used to model the linear relationship between a scalar or vector response and one or more explanatory variables. 

$$ 
 y_{i}=\beta_{0}+\beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\ldots+\beta_{p-1}x_{i,p-1}+\epsilon_{i}. 
$$

where $y_i$ is (are) the response variable(s) for each unit ($i$), $x_{i, p-1}$ are the explanatory variables, and $\beta_{p-1}$ are the parameter coefficients. The errors, $\epsilon_i$, are assumed to be normally distributed with mean 0 and constant variance $\sigma^2$. In this approach one would find estimates for the $\beta_{p-1}$ parameters using values that minimize the sum of squared errors for the sample.

## Generalized Linear Regression Model

In generalized linear models, the response variable $y_i$ is now assumed to follow an exponential family distribution with mean $\mu_i$, which is assumed to be some (often nonlinear) function of $x_i^T\beta$. Note that the covariates affect the distribution of 
$y_i$ only through the linear combination $x_i^T\beta$.

The general form, written now in matrix multiplication format, is:
$$
\begin{gathered}
g(\mu)=\eta=X \beta \\
E(y)=\mu=g^{-1}(\eta)
\end{gathered}
$$
where $g(.)$ is a link function relating the mean $\mu$ to the linear predictor(s) $X \beta$ (also denoted by $\eta$). Recall in linear regression we assume a Gaussian (i.e. normal) distribution for the response, we assume equal variance for all observations, and that there is a direct link of the linear predictor and the expected value $\mu$, i.e. $\mu = X\beta$. As such, the typical linear regression model is a generalized linear model with a Gaussian distribution and ‘identity’ link function.

For count data, a Poisson distribution is used. There is only one parameter to be considered, $\lambda$, since for the Poisson the mean and variance are equal. For the Poisson, the (canonical) link function $g(.)$, is the natural log, and so relates the log of $\lambda$ to the linear predictor. As such we could also write it in terms of exponentiating the right-hand side.

$$
\begin{gathered}
y \sim \mathcal{P}(\lambda) \\
\ln (\lambda)=b_{0}+b_{1} \cdot x_{1}+b_{2} \cdot x_{2} \ldots+b_{p} \cdot x_{p} \\
\lambda=e^{b_{0}+b_{1} \cdot x_{1}+b_{2} \cdot x_{2} \ldots+b_{p} \cdot x_{p}}
\end{gathered}
$$
Generalized linear models (and linear models as a subset) have a handful of tools to adapt to data that do not have quite so straightforward a relationship with associated covariates. Transformations to covariates that allow for inclusion of polynomial terms (e.g. quadratic, cubic, etc) are useful but have their own limits. 

## Generalized Additive Model

Generalized additive models (GAMs) are statistical models that can be used to estimate trends as smooth functions of time. This form allows for the now nonlinear predictor(s) to relate to the expected value, with whatever link function may be appropriate.

$$
\begin{gathered}
y \sim \operatorname{ExpoFam}(\mu, \text { etc. }) \\
E(y)=\mu \\
g(\mu)=b_{0}+f\left(x_{1}\right)+f\left(x_{2}\right) \ldots+f\left(x_{p}\right)
\end{gathered}
$$
This approach is similar to GLM, but now instead of parametric coefficients on each of the variables, we now have smoothing functions ($f$) which are very flexible in their forms. Note that we can still include some covariates that do linearly relate to the response variable ($y$). 

A spline is a function defined piece-wise by polynomials. Each consist of smaller basis functions, of which we may choose between several types/forms based on the data. We may also choose how many "pieces" to use by defining the number of knots ($k$) for each spline.

Spline Fun Fact: The term spline comes from the flexible devices used by shipbuilders and draftspersons to draw smooth shapes. Thanks, Wikipedia.

To model a potentially nonlinear smooth or surface, three different smooth functions are available:

 * s() : for modeling a 1-dimensional smooth, or
for modeling isotropic interactions (variables are measured in same units and on same scale)
 * te(): for modeling 2- or n-dimensional interaction surfaces of variables that are not isotropic (but see info about d parameter below). Includes ‘main’ effects.
 * ti(): for modeling 2- or n-dimensional interaction surfaces that do not include the ‘main effects’.

### Basis Functions

There are several smoothing bases $b$ (splines) which are suitable for regression:

* thin plate regression splines
* cubic regression spline
* cyclic cubic regression spline
* P-splines

### Generalized Additive Mixture Model

### Hierarchical Generalized Additive Models

## Summary


## Additional Resources

This summary pulls heavily from the following resources that are very useful for a deeper dive into these models. 

* [Generalized Additive Models](https://m-clark.github.io/generalized-additive-models/)




