# GA(M)M Overview

```{r, echo=FALSE, message=F, warning=F}
library(dplyr)
library(here)
library(lubridate)
library(ggplot2)
library(magrittr)
library(mgcv)

source(here("scripts/01-LoadData.R"))

#helps to format this document 
#by default the included code chunks do not appear in the report view
knitr::opts_chunk$set(echo = F, 
                      eval = F,
                      message = F, 
                      cache=TRUE, 
                      warning=FALSE, 
                      error=FALSE, 
                      root.dir = normalizePath(".."))

```

# Overview of Generalized Additive (Mixture) Models

## Linear Regression Model

Linear regression is used to model the linear relationship between a scalar or vector response and one or more explanatory variables. 

$$ 
 y_{i}=\beta_{0}+\beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\ldots+\beta_{p-1}x_{i,p-1}+\epsilon_{i}. 
$$

where $y_i$ is (are) the response variable(s) for each unit ($i$), $x_{i, p-1}$ are the explanatory variables, and $\beta_{p-1}$ are the parameter coefficients. The errors, $\epsilon_i$, are assumed to be normally distributed with mean 0 and constant variance $\sigma^2$. In this approach one would find estimates for the $\beta_{p-1}$ parameters using values that minimize the sum of squared errors for the sample.

## Generalized Linear Regression Model

In generalized linear models, the response variable $y_i$ is now assumed to follow an exponential family distribution with mean $\mu_i$, which is assumed to be some (often nonlinear) function of $x_i^T\beta$. Note that the covariates affect the distribution of 
$y_i$ only through the linear combination $x_i^T\beta$.

The general form, written now in matrix multiplication format, is:
$$
\begin{gathered}
g(\mu)=\eta=X \beta \\
E(y)=\mu=g^{-1}(\eta)
\end{gathered}
$$
where $g(.)$ is a link function relating the mean $\mu$ to the linear predictor(s) $X \beta$ (also denoted by $\eta$). Recall in linear regression we assume a Gaussian (i.e. normal) distribution for the response, we assume equal variance for all observations, and that there is a direct link of the linear predictor and the expected value $\mu$, i.e. $\mu = X\beta$. As such, the typical linear regression model is a generalized linear model with a Gaussian distribution and ‘identity’ link function.

For count data, a Poisson distribution is used. There is only one parameter to be considered, $\lambda$, since for the Poisson the mean and variance are equal. For the Poisson, the (canonical) link function $g(.)$, is the natural log, and so relates the log of $\lambda$ to the linear predictor. As such we could also write it in terms of exponentiating the right-hand side.

$$
\begin{gathered}
y \sim \mathcal{P}(\lambda) \\
\ln (\lambda)=b_{0}+b_{1} \cdot x_{1}+b_{2} \cdot x_{2} \ldots+b_{p} \cdot x_{p} \\
\lambda=e^{b_{0}+b_{1} \cdot x_{1}+b_{2} \cdot x_{2} \ldots+b_{p} \cdot x_{p}}
\end{gathered}
$$
Generalized linear models (and linear models as a subset) have a handfull of tools to adapt to data that do not have quite so straightforward a relationship with associated covariates. Transformations to covariates that allow for inclusion of polynomial terms (e.g. quadratic, cubic, etc) are useful but have their own limits. 

## Generalized Additive Model

Generalised additive models (GAMs) are statistical models that can be used to estimate trends as smooth functions of time. This form allows for the now nonlinear predictor(s) to relate to the expected value, with whatever link function may be appropriate.

$$
\begin{gathered}
y \sim \operatorname{ExpoFam}(\mu, \text { etc. }) \\
E(y)=\mu \\
g(\mu)=b_{0}+f\left(x_{1}\right)+f\left(x_{2}\right) \ldots+f\left(x_{p}\right)
\end{gathered}
$$
This approach is similar to GLM, but now instead of parametric coefficients on each of the variables, we now have smoothing functions ($f$) which are very flexible in their forms. Note that we can still include some covariates that do linearly relate to the response variable ($y$). 

### Basis Functions

There are several smoothing bases $b$ (splines) which are suitable for regression:

* thin plate regression splines
* cubic regression spline
* cyclic cubic regression spline
* P-splines


## Why not a Time Series Approach?

## Assumptions Made

## Limitations

### Missing Covariate Data
